{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import requests\n",
    "from selenium.common.exceptions import StaleElementReferenceException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create directory\n",
    "def make_directory(dirname):\n",
    "    current_path=os.getcwd()\n",
    "    path=os.path.join(current_path,dirname)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    #print(\"Dircetory {0} has been created \\n\\n\".format(dirname))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRNAME=\"Flipkart_men_formal_trouser\"\n",
    "make_directory(dirname=DIRNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_images(driver):\n",
    "    image=driver.find_elements_by_xpath('//img[@class=\"_3togXc\"]')\n",
    "    #image=driver.find_elements_by_xpath(im)\n",
    "    url={}\n",
    "    url['image_urls']=[]\n",
    "    for i in image:\n",
    "        source=i.get_attribute('src')\n",
    "        url['image_urls'].append(source)\n",
    "    return url \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(data,dirname,page):\n",
    "    \n",
    "    for index , link in enumerate(data['image_urls']):\n",
    "        print(\"Downloading {0} of {1} image\".format(index+1,len(data['image_urls'])))\n",
    "        response=requests.get(link)\n",
    "        #with open('Flipkart_men_formal_trouser/img_{0}.jpeg'.format(index),\"wb\") as file:\n",
    "        with open('{0}/img_{1}{2}.jpeg'.format(dirname, page,index),\"wb\") as file:\n",
    "            file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(executable_path='D:/scraping/chromedriver.exe')\n",
    "current_page_url=driver.get('https://www.flipkart.com/mens-trousers/pr?sid=clo%2Cvua%2Cmle%2Clhk&otracker%5B%5D=categorytree&otracker%5B%5D=nmenu_sub_Men_0_Formal+Trousers&p%5B%5D=facets.occasion%255B%255D%3DFormal&page=1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping the page 1 of 4 pages\n",
      "Current page scraped [<selenium.webdriver.remote.webelement.WebElement (session=\"264bd4577b9aabd94e12c8c0a1b558e2\", element=\"23baf839-f670-4403-8c8e-ee9f25a3d5e3\")>]Done\n",
      "Downloading 1 of 40 image\n",
      "Downloading 2 of 40 image\n",
      "Downloading 3 of 40 image\n",
      "Downloading 4 of 40 image\n",
      "Downloading 5 of 40 image\n",
      "Downloading 6 of 40 image\n",
      "Downloading 7 of 40 image\n",
      "Downloading 8 of 40 image\n",
      "Downloading 9 of 40 image\n",
      "Downloading 10 of 40 image\n",
      "Downloading 11 of 40 image\n",
      "Downloading 12 of 40 image\n",
      "Downloading 13 of 40 image\n",
      "Downloading 14 of 40 image\n",
      "Downloading 15 of 40 image\n",
      "Downloading 16 of 40 image\n",
      "Downloading 17 of 40 image\n",
      "Downloading 18 of 40 image\n",
      "Downloading 19 of 40 image\n",
      "Downloading 20 of 40 image\n",
      "Downloading 21 of 40 image\n",
      "Downloading 22 of 40 image\n",
      "Downloading 23 of 40 image\n",
      "Downloading 24 of 40 image\n",
      "Downloading 25 of 40 image\n",
      "Downloading 26 of 40 image\n",
      "Downloading 27 of 40 image\n",
      "Downloading 28 of 40 image\n",
      "Downloading 29 of 40 image\n",
      "Downloading 30 of 40 image\n",
      "Downloading 31 of 40 image\n",
      "Downloading 32 of 40 image\n",
      "Downloading 33 of 40 image\n",
      "Downloading 34 of 40 image\n",
      "Downloading 35 of 40 image\n",
      "Downloading 36 of 40 image\n",
      "Downloading 37 of 40 image\n",
      "Downloading 38 of 40 image\n",
      "Downloading 39 of 40 image\n",
      "Downloading 40 of 40 image\n",
      "Scraping of page 1 done \n",
      "Moving the next page\n",
      "the new page is 3\n",
      "Scraping the page 2 of 4 pages\n",
      "Current page scraped [<selenium.webdriver.remote.webelement.WebElement (session=\"264bd4577b9aabd94e12c8c0a1b558e2\", element=\"23baf839-f670-4403-8c8e-ee9f25a3d5e3\")>]Done\n",
      "Downloading 1 of 40 image\n",
      "Downloading 2 of 40 image\n",
      "Downloading 3 of 40 image\n",
      "Downloading 4 of 40 image\n",
      "Downloading 5 of 40 image\n",
      "Downloading 6 of 40 image\n",
      "Downloading 7 of 40 image\n",
      "Downloading 8 of 40 image\n",
      "Downloading 9 of 40 image\n",
      "Downloading 10 of 40 image\n",
      "Downloading 11 of 40 image\n",
      "Downloading 12 of 40 image\n",
      "Downloading 13 of 40 image\n",
      "Downloading 14 of 40 image\n",
      "Downloading 15 of 40 image\n",
      "Downloading 16 of 40 image\n",
      "Downloading 17 of 40 image\n",
      "Downloading 18 of 40 image\n",
      "Downloading 19 of 40 image\n",
      "Downloading 20 of 40 image\n",
      "Downloading 21 of 40 image\n",
      "Downloading 22 of 40 image\n",
      "Downloading 23 of 40 image\n",
      "Downloading 24 of 40 image\n",
      "Downloading 25 of 40 image\n",
      "Downloading 26 of 40 image\n",
      "Downloading 27 of 40 image\n",
      "Downloading 28 of 40 image\n",
      "Downloading 29 of 40 image\n",
      "Downloading 30 of 40 image\n",
      "Downloading 31 of 40 image\n",
      "Downloading 32 of 40 image\n",
      "Downloading 33 of 40 image\n",
      "Downloading 34 of 40 image\n",
      "Downloading 35 of 40 image\n",
      "Downloading 36 of 40 image\n",
      "Downloading 37 of 40 image\n",
      "Downloading 38 of 40 image\n",
      "Downloading 39 of 40 image\n",
      "Downloading 40 of 40 image\n",
      "Scraping of page 2 done \n",
      "Moving the next page\n",
      "the new page is 4\n",
      "We are facing unwanted errors:\n",
      "\n",
      "\n",
      "Exceptions faced at\n",
      "scraping 3 page of 4 pages\n",
      "The current page scraped is 5\n",
      "Scraping the page 4 of 4 pages\n",
      "Current page scraped [<selenium.webdriver.remote.webelement.WebElement (session=\"264bd4577b9aabd94e12c8c0a1b558e2\", element=\"3d686857-7bcd-4179-884e-7eebd09513f6\")>]Done\n",
      "Downloading 1 of 40 image\n",
      "Downloading 2 of 40 image\n",
      "Downloading 3 of 40 image\n",
      "Downloading 4 of 40 image\n",
      "Downloading 5 of 40 image\n",
      "Downloading 6 of 40 image\n",
      "Downloading 7 of 40 image\n",
      "Downloading 8 of 40 image\n",
      "Downloading 9 of 40 image\n",
      "Downloading 10 of 40 image\n",
      "Downloading 11 of 40 image\n",
      "Downloading 12 of 40 image\n",
      "Downloading 13 of 40 image\n",
      "Downloading 14 of 40 image\n",
      "Downloading 15 of 40 image\n",
      "Downloading 16 of 40 image\n",
      "Downloading 17 of 40 image\n",
      "Downloading 18 of 40 image\n",
      "Downloading 19 of 40 image\n",
      "Downloading 20 of 40 image\n",
      "Downloading 21 of 40 image\n",
      "Downloading 22 of 40 image\n",
      "Downloading 23 of 40 image\n",
      "Downloading 24 of 40 image\n",
      "Downloading 25 of 40 image\n",
      "Downloading 26 of 40 image\n",
      "Downloading 27 of 40 image\n",
      "Downloading 28 of 40 image\n",
      "Downloading 29 of 40 image\n",
      "Downloading 30 of 40 image\n",
      "Downloading 31 of 40 image\n",
      "Downloading 32 of 40 image\n",
      "Downloading 33 of 40 image\n",
      "Downloading 34 of 40 image\n",
      "Downloading 35 of 40 image\n",
      "Downloading 36 of 40 image\n",
      "Downloading 37 of 40 image\n",
      "Downloading 38 of 40 image\n",
      "Downloading 39 of 40 image\n",
      "Downloading 40 of 40 image\n",
      "Scraping of page 4 done \n",
      "Moving the next page\n",
      "the new page is 5\n"
     ]
    }
   ],
   "source": [
    "start=1\n",
    "end=4\n",
    "for page in range(start, end+1):\n",
    "    try:\n",
    "        urls=scrap_images(driver=driver)\n",
    "        print(\"Scraping the page {0} of {1} pages\".format(page, end))\n",
    "        page_value=driver.find_elements_by_xpath(\"//a[@class='_2Xp0TH fyt9Eu']\")\n",
    "        print(\"Current page scraped {0}Done\".format(page_value))\n",
    "        \n",
    "        save_images(data=urls, dirname=DIRNAME,page=page)\n",
    "        print(\"Scraping of page {0} done \".format(page))\n",
    "        \n",
    "        print(\"Moving the next page\")\n",
    "        button_type=driver.find_element_by_xpath(\"//div[@class='_2zg3yZ']//a[@class='_3fVaIS']//span\").get_attribute('innerHTML')\n",
    "        if button_type==\"Next\":\n",
    "            driver.find_element_by_xpath(\"//a[@class='_3fVaIS']\").click()\n",
    "        else:\n",
    "            driver.find_element_by_xpath(\"//a[@class='_3fVaIS'][2]\").click()\n",
    "        new_page=driver.find_element_by_xpath(\"//a[@class='_2Xp0TH fyt9Eu']\").text\n",
    "        print(\"the new page is {}\".format(new_page))\n",
    "\n",
    "        \n",
    "    except StaleElementReferenceException as Exception:\n",
    "        print(\"We are facing unwanted errors:\\n\\n\")\n",
    "        error_page=driver.find_element_by_xpath(\"//a[@class='_2Xp0TH fyt9Eu']\").text\n",
    "        print(\"Exceptions faced at\".format(error_page))\n",
    "        value=driver.find_element_by_xpath(\"//a[@class='_2Xp0TH fyt9Eu']\")\n",
    "        link=value.get_attribute('href')\n",
    "        driver.get(link)\n",
    "        \n",
    "        urls=scrap_images(driver=driver)\n",
    "        print(\"scraping {0} page of {1} pages\".format(page,end))\n",
    "        page_value=driver.find_element_by_xpath(\"//a[@class='_2Xp0TH fyt9Eu']\").text\n",
    "        print(\"The current page scraped is {}\".format(page_value))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
